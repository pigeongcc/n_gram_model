# N-граммная модель
Генерации текста с помощью N-граммной модели и кластеризации.


### Запуск
Обучение:
```
python3 train.py --input-dir /path/to_folder/with_txt_files --model /path/to_save/model.pkl --N 2
```

Генерация текста:
```
python3 generate.py --model /path/to/model.pkl --prefix "Текст начнется с этой строки" --length 50
```


### Примеры работы
Примеры работы биграммы, обученной на романе М.Ю.Лермонтова "Герой нашего времени":

> Я настолько преисполнился в своем сознании что оно гораздо выше чем у нас на оскорбление личности. Ответом на долину. Длины. Покушении на станции оставалось еще шла в тележку на кавказе. Брань не было господин - с помощью этих осетин. Горло распевал песни. Этим кончился и шапка. Счастию для меня.

> Он был герой нашего поколения в гору хотя уже оттуда до ночи взобраться на оскорбление личности. Критики. Звук не переедешь. Мечту сделаться исправителем людских пороков. Оправданием и боже его понимает басни если в серебро. Туман нахлынувший волнами из одного небольшого чемодана который до половины был подпоручиком - завтра.


### Интересные моменты
- Модель принимает параметр N, т.е. можно обучить биграмму, триграмму, и т.д.
- ML-надстройка заключается в кластеризации слов из текстового корпуса по признакам: часть речи, число. После кластеризации строится матрица размера C*C (С - количество кластеров, заранее неизвестно), содержащая частоту появления в тексте слова из кластера X вслед за словом из кластера Y. Информация из этой матрицы учитывается при выборе следующего слова во время генерации текста. Таким образом, ML в этой задаче выделяет информацию, какие часть речи и число вероятнее встретятся в следующем слове.
- Каждое следующее слово генерируется выбором пачки (например, 5 слов) методом roulette wheel из наиболее вероятных в данной N-грамме. Затем из пачки выбирается слово с наибольшей кластерной частотой, т.е. самое вероятное по мнению ML-модели.


### Пространство для улучшения
- Завести больше признаков для слов в ML-модели: род, падеж и др. Как для подбора следующего слова, так и для его коррекции.
- Обернуть модель и датасет в классы.


### Проблемы, с которыми я столкнулся
- Модель ест много памяти. Для N-граммной модели, обученной на корпусе с X уникальных слов, будет создана матрица объёмом 8*X^N байт. Я решил поиграть с типами данных: уйдя от np.float64 к np.uint8 и применив квантование, уменьшил объём матрицы до X^N байт. Тем не менее, это всё ещё много. Интересно, как это решено в библиотеке nltk: их моделька может кушать большие тексты.


### Альтернативные решения
- Можно попробовать векторизовать n-граммы через Pointwise Mutual Information. Правда, не думаю, что на маленьком датасете будет прирост качества генерации.
- Вместо n-грамм можно использовать коллокации. Это должно прибавить вариативности в генерацию.
